{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aditya/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/aditya/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-3-640e33fe9da4>:1: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /home/aditya/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /home/aditya/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:252: _internal_retry.<locals>.wrap.<locals>.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use urllib or similar directly.\n",
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "WARNING:tensorflow:From /home/aditya/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "WARNING:tensorflow:From /home/aditya/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /home/aditya/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /home/aditya/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets(\"/tmp/data\", one_hot = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "epochs = 500\n",
    "batch_size = 128\n",
    "\n",
    "n_hidden_1 = 256\n",
    "n_hidden_2 = 256\n",
    "num_input = 784\n",
    "num_classes = 10\n",
    "\n",
    "X = tf.placeholder(\"float\", shape = [None, num_input])\n",
    "Y = tf.placeholder(\"float\", shape = [None, num_classes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = {\n",
    "    'l1':tf.Variable(tf.random_normal([num_input, n_hidden_1])),\n",
    "    'l2':tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
    "    'o':tf.Variable(tf.random_normal([n_hidden_2, num_classes])),\n",
    "}\n",
    "\n",
    "bias = {\n",
    "    'b1':tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'b2':tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "    'b3':tf.Variable(tf.random_normal([num_classes])),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neural_net(x):\n",
    "    layer1 = tf.add(tf.matmul(x, weights['l1']), bias['b1'])\n",
    "    layer2 = tf.add(tf.matmul(layer1, weights['l2']), bias['b2'])\n",
    "    output = tf.matmul(layer2, weights['o']) + bias['b3']\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-31-42545cc34af9>:3: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logits = neural_net(X)\n",
    "\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = logits, labels = Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(loss)\n",
    "\n",
    "correct_pred = tf.equal(tf.argmax(logits,1), tf.argmax(Y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 ..... Loss: 3201.586181640625 ..... Accuracy: 0.1171875\n",
      "Epoch: 2 ..... Loss: 3087.5693359375 ..... Accuracy: 0.1328125\n",
      "Epoch: 3 ..... Loss: 2812.78173828125 ..... Accuracy: 0.109375\n",
      "Epoch: 4 ..... Loss: 2875.694580078125 ..... Accuracy: 0.09375\n",
      "Epoch: 5 ..... Loss: 2717.284912109375 ..... Accuracy: 0.09375\n",
      "Epoch: 6 ..... Loss: 2166.12109375 ..... Accuracy: 0.15625\n",
      "Epoch: 7 ..... Loss: 2416.142822265625 ..... Accuracy: 0.0859375\n",
      "Epoch: 8 ..... Loss: 2031.89990234375 ..... Accuracy: 0.1484375\n",
      "Epoch: 9 ..... Loss: 1883.725341796875 ..... Accuracy: 0.15625\n",
      "Epoch: 10 ..... Loss: 1946.557861328125 ..... Accuracy: 0.1328125\n",
      "Epoch: 11 ..... Loss: 1908.734375 ..... Accuracy: 0.1875\n",
      "Epoch: 12 ..... Loss: 2004.01953125 ..... Accuracy: 0.171875\n",
      "Epoch: 13 ..... Loss: 1691.3304443359375 ..... Accuracy: 0.2265625\n",
      "Epoch: 14 ..... Loss: 1620.3758544921875 ..... Accuracy: 0.3359375\n",
      "Epoch: 15 ..... Loss: 1549.128173828125 ..... Accuracy: 0.2265625\n",
      "Epoch: 16 ..... Loss: 1503.753173828125 ..... Accuracy: 0.2421875\n",
      "Epoch: 17 ..... Loss: 1256.4188232421875 ..... Accuracy: 0.25\n",
      "Epoch: 18 ..... Loss: 1409.552490234375 ..... Accuracy: 0.3203125\n",
      "Epoch: 19 ..... Loss: 1513.3193359375 ..... Accuracy: 0.265625\n",
      "Epoch: 20 ..... Loss: 1201.614990234375 ..... Accuracy: 0.390625\n",
      "Epoch: 21 ..... Loss: 1034.0419921875 ..... Accuracy: 0.3671875\n",
      "Epoch: 22 ..... Loss: 1310.7113037109375 ..... Accuracy: 0.3359375\n",
      "Epoch: 23 ..... Loss: 936.392578125 ..... Accuracy: 0.4609375\n",
      "Epoch: 24 ..... Loss: 1126.9400634765625 ..... Accuracy: 0.3671875\n",
      "Epoch: 25 ..... Loss: 996.697998046875 ..... Accuracy: 0.4140625\n",
      "Epoch: 26 ..... Loss: 1116.35107421875 ..... Accuracy: 0.4140625\n",
      "Epoch: 27 ..... Loss: 820.862060546875 ..... Accuracy: 0.453125\n",
      "Epoch: 28 ..... Loss: 996.3301391601562 ..... Accuracy: 0.40625\n",
      "Epoch: 29 ..... Loss: 957.9801025390625 ..... Accuracy: 0.4296875\n",
      "Epoch: 30 ..... Loss: 914.0194702148438 ..... Accuracy: 0.4296875\n",
      "Epoch: 31 ..... Loss: 784.0240478515625 ..... Accuracy: 0.5\n",
      "Epoch: 32 ..... Loss: 947.0037841796875 ..... Accuracy: 0.4765625\n",
      "Epoch: 33 ..... Loss: 913.4962768554688 ..... Accuracy: 0.4296875\n",
      "Epoch: 34 ..... Loss: 749.4426879882812 ..... Accuracy: 0.515625\n",
      "Epoch: 35 ..... Loss: 1073.997314453125 ..... Accuracy: 0.390625\n",
      "Epoch: 36 ..... Loss: 772.1851806640625 ..... Accuracy: 0.5703125\n",
      "Epoch: 37 ..... Loss: 765.74755859375 ..... Accuracy: 0.578125\n",
      "Epoch: 38 ..... Loss: 703.77587890625 ..... Accuracy: 0.5\n",
      "Epoch: 39 ..... Loss: 943.522216796875 ..... Accuracy: 0.5234375\n",
      "Epoch: 40 ..... Loss: 679.8065795898438 ..... Accuracy: 0.5234375\n",
      "Epoch: 41 ..... Loss: 702.3817749023438 ..... Accuracy: 0.5546875\n",
      "Epoch: 42 ..... Loss: 519.9965209960938 ..... Accuracy: 0.6171875\n",
      "Epoch: 43 ..... Loss: 754.15673828125 ..... Accuracy: 0.5390625\n",
      "Epoch: 44 ..... Loss: 507.85308837890625 ..... Accuracy: 0.65625\n",
      "Epoch: 45 ..... Loss: 667.5064697265625 ..... Accuracy: 0.53125\n",
      "Epoch: 46 ..... Loss: 663.0843505859375 ..... Accuracy: 0.53125\n",
      "Epoch: 47 ..... Loss: 626.9257202148438 ..... Accuracy: 0.5546875\n",
      "Epoch: 48 ..... Loss: 735.8504028320312 ..... Accuracy: 0.53125\n",
      "Epoch: 49 ..... Loss: 474.97607421875 ..... Accuracy: 0.6796875\n",
      "Epoch: 50 ..... Loss: 602.7713623046875 ..... Accuracy: 0.5390625\n",
      "Epoch: 51 ..... Loss: 699.858642578125 ..... Accuracy: 0.5546875\n",
      "Epoch: 52 ..... Loss: 511.05023193359375 ..... Accuracy: 0.640625\n",
      "Epoch: 53 ..... Loss: 594.3662109375 ..... Accuracy: 0.6171875\n",
      "Epoch: 54 ..... Loss: 540.1076049804688 ..... Accuracy: 0.578125\n",
      "Epoch: 55 ..... Loss: 472.985595703125 ..... Accuracy: 0.6015625\n",
      "Epoch: 56 ..... Loss: 580.6633911132812 ..... Accuracy: 0.625\n",
      "Epoch: 57 ..... Loss: 655.0263671875 ..... Accuracy: 0.5390625\n",
      "Epoch: 58 ..... Loss: 547.1559448242188 ..... Accuracy: 0.609375\n",
      "Epoch: 59 ..... Loss: 615.3821411132812 ..... Accuracy: 0.5703125\n",
      "Epoch: 60 ..... Loss: 606.564697265625 ..... Accuracy: 0.59375\n",
      "Epoch: 61 ..... Loss: 461.901611328125 ..... Accuracy: 0.578125\n",
      "Epoch: 62 ..... Loss: 462.52667236328125 ..... Accuracy: 0.640625\n",
      "Epoch: 63 ..... Loss: 525.2511596679688 ..... Accuracy: 0.609375\n",
      "Epoch: 64 ..... Loss: 511.37945556640625 ..... Accuracy: 0.6015625\n",
      "Epoch: 65 ..... Loss: 519.676025390625 ..... Accuracy: 0.65625\n",
      "Epoch: 66 ..... Loss: 476.772705078125 ..... Accuracy: 0.6171875\n",
      "Epoch: 67 ..... Loss: 369.15948486328125 ..... Accuracy: 0.703125\n",
      "Epoch: 68 ..... Loss: 352.0352783203125 ..... Accuracy: 0.734375\n",
      "Epoch: 69 ..... Loss: 432.9122619628906 ..... Accuracy: 0.671875\n",
      "Epoch: 70 ..... Loss: 338.97076416015625 ..... Accuracy: 0.7265625\n",
      "Epoch: 71 ..... Loss: 254.8117218017578 ..... Accuracy: 0.7421875\n",
      "Epoch: 72 ..... Loss: 360.066650390625 ..... Accuracy: 0.6953125\n",
      "Epoch: 73 ..... Loss: 400.08306884765625 ..... Accuracy: 0.65625\n",
      "Epoch: 74 ..... Loss: 454.80853271484375 ..... Accuracy: 0.578125\n",
      "Epoch: 75 ..... Loss: 501.5126647949219 ..... Accuracy: 0.703125\n",
      "Epoch: 76 ..... Loss: 292.2128601074219 ..... Accuracy: 0.765625\n",
      "Epoch: 77 ..... Loss: 468.134765625 ..... Accuracy: 0.6640625\n",
      "Epoch: 78 ..... Loss: 423.58612060546875 ..... Accuracy: 0.6953125\n",
      "Epoch: 79 ..... Loss: 504.472412109375 ..... Accuracy: 0.640625\n",
      "Epoch: 80 ..... Loss: 364.02197265625 ..... Accuracy: 0.6953125\n",
      "Epoch: 81 ..... Loss: 289.731689453125 ..... Accuracy: 0.71875\n",
      "Epoch: 82 ..... Loss: 337.83758544921875 ..... Accuracy: 0.7109375\n",
      "Epoch: 83 ..... Loss: 294.24102783203125 ..... Accuracy: 0.671875\n",
      "Epoch: 84 ..... Loss: 261.2013244628906 ..... Accuracy: 0.7578125\n",
      "Epoch: 85 ..... Loss: 267.8585205078125 ..... Accuracy: 0.734375\n",
      "Epoch: 86 ..... Loss: 374.57891845703125 ..... Accuracy: 0.640625\n",
      "Epoch: 87 ..... Loss: 343.6986083984375 ..... Accuracy: 0.6875\n",
      "Epoch: 88 ..... Loss: 338.4500732421875 ..... Accuracy: 0.734375\n",
      "Epoch: 89 ..... Loss: 332.16412353515625 ..... Accuracy: 0.703125\n",
      "Epoch: 90 ..... Loss: 277.63836669921875 ..... Accuracy: 0.71875\n",
      "Epoch: 91 ..... Loss: 309.6639404296875 ..... Accuracy: 0.6953125\n",
      "Epoch: 92 ..... Loss: 311.40966796875 ..... Accuracy: 0.7578125\n",
      "Epoch: 93 ..... Loss: 504.8270263671875 ..... Accuracy: 0.640625\n",
      "Epoch: 94 ..... Loss: 323.5163879394531 ..... Accuracy: 0.7734375\n",
      "Epoch: 95 ..... Loss: 371.55145263671875 ..... Accuracy: 0.7421875\n",
      "Epoch: 96 ..... Loss: 307.79046630859375 ..... Accuracy: 0.7421875\n",
      "Epoch: 97 ..... Loss: 331.8768005371094 ..... Accuracy: 0.71875\n",
      "Epoch: 98 ..... Loss: 321.08172607421875 ..... Accuracy: 0.6953125\n",
      "Epoch: 99 ..... Loss: 227.0963592529297 ..... Accuracy: 0.7734375\n",
      "Epoch: 100 ..... Loss: 387.2914733886719 ..... Accuracy: 0.640625\n",
      "Epoch: 101 ..... Loss: 227.3915557861328 ..... Accuracy: 0.7734375\n",
      "Epoch: 102 ..... Loss: 266.9984130859375 ..... Accuracy: 0.765625\n",
      "Epoch: 103 ..... Loss: 238.10507202148438 ..... Accuracy: 0.796875\n",
      "Epoch: 104 ..... Loss: 338.60748291015625 ..... Accuracy: 0.7421875\n",
      "Epoch: 105 ..... Loss: 167.47291564941406 ..... Accuracy: 0.8046875\n",
      "Epoch: 106 ..... Loss: 301.11407470703125 ..... Accuracy: 0.7109375\n",
      "Epoch: 107 ..... Loss: 310.14483642578125 ..... Accuracy: 0.703125\n",
      "Epoch: 108 ..... Loss: 258.63848876953125 ..... Accuracy: 0.765625\n",
      "Epoch: 109 ..... Loss: 257.3216552734375 ..... Accuracy: 0.78125\n",
      "Epoch: 110 ..... Loss: 236.26669311523438 ..... Accuracy: 0.8125\n",
      "Epoch: 111 ..... Loss: 346.228271484375 ..... Accuracy: 0.765625\n",
      "Epoch: 112 ..... Loss: 327.0335693359375 ..... Accuracy: 0.734375\n",
      "Epoch: 113 ..... Loss: 363.14434814453125 ..... Accuracy: 0.7421875\n",
      "Epoch: 114 ..... Loss: 258.3818664550781 ..... Accuracy: 0.7734375\n",
      "Epoch: 115 ..... Loss: 246.6596221923828 ..... Accuracy: 0.796875\n",
      "Epoch: 116 ..... Loss: 371.06768798828125 ..... Accuracy: 0.75\n",
      "Epoch: 117 ..... Loss: 242.81219482421875 ..... Accuracy: 0.7734375\n",
      "Epoch: 118 ..... Loss: 141.70758056640625 ..... Accuracy: 0.8046875\n",
      "Epoch: 119 ..... Loss: 352.958984375 ..... Accuracy: 0.71875\n",
      "Epoch: 120 ..... Loss: 232.76309204101562 ..... Accuracy: 0.8203125\n",
      "Epoch: 121 ..... Loss: 344.9981994628906 ..... Accuracy: 0.75\n",
      "Epoch: 122 ..... Loss: 239.8714599609375 ..... Accuracy: 0.765625\n",
      "Epoch: 123 ..... Loss: 280.0419921875 ..... Accuracy: 0.75\n",
      "Epoch: 124 ..... Loss: 232.3434295654297 ..... Accuracy: 0.8046875\n",
      "Epoch: 125 ..... Loss: 253.05941772460938 ..... Accuracy: 0.765625\n",
      "Epoch: 126 ..... Loss: 220.18247985839844 ..... Accuracy: 0.7734375\n",
      "Epoch: 127 ..... Loss: 327.0732421875 ..... Accuracy: 0.7578125\n",
      "Epoch: 128 ..... Loss: 303.8079833984375 ..... Accuracy: 0.765625\n",
      "Epoch: 129 ..... Loss: 231.96702575683594 ..... Accuracy: 0.71875\n",
      "Epoch: 130 ..... Loss: 192.1901092529297 ..... Accuracy: 0.7734375\n",
      "Epoch: 131 ..... Loss: 320.6781921386719 ..... Accuracy: 0.7578125\n",
      "Epoch: 132 ..... Loss: 342.2447814941406 ..... Accuracy: 0.7421875\n",
      "Epoch: 133 ..... Loss: 315.0564880371094 ..... Accuracy: 0.7265625\n",
      "Epoch: 134 ..... Loss: 307.5625305175781 ..... Accuracy: 0.7578125\n",
      "Epoch: 135 ..... Loss: 244.9212646484375 ..... Accuracy: 0.7421875\n",
      "Epoch: 136 ..... Loss: 314.68804931640625 ..... Accuracy: 0.7109375\n",
      "Epoch: 137 ..... Loss: 241.407470703125 ..... Accuracy: 0.7578125\n",
      "Epoch: 138 ..... Loss: 238.39425659179688 ..... Accuracy: 0.796875\n",
      "Epoch: 139 ..... Loss: 352.0610656738281 ..... Accuracy: 0.75\n",
      "Epoch: 140 ..... Loss: 259.28668212890625 ..... Accuracy: 0.78125\n",
      "Epoch: 141 ..... Loss: 305.2966003417969 ..... Accuracy: 0.7421875\n",
      "Epoch: 142 ..... Loss: 284.1027526855469 ..... Accuracy: 0.796875\n",
      "Epoch: 143 ..... Loss: 374.8322448730469 ..... Accuracy: 0.734375\n",
      "Epoch: 144 ..... Loss: 221.63063049316406 ..... Accuracy: 0.8125\n",
      "Epoch: 145 ..... Loss: 150.60589599609375 ..... Accuracy: 0.828125\n",
      "Epoch: 146 ..... Loss: 276.69818115234375 ..... Accuracy: 0.7421875\n",
      "Epoch: 147 ..... Loss: 280.9304504394531 ..... Accuracy: 0.765625\n",
      "Epoch: 148 ..... Loss: 199.13296508789062 ..... Accuracy: 0.8046875\n",
      "Epoch: 149 ..... Loss: 320.03076171875 ..... Accuracy: 0.75\n",
      "Epoch: 150 ..... Loss: 143.69525146484375 ..... Accuracy: 0.796875\n",
      "Epoch: 151 ..... Loss: 242.1195068359375 ..... Accuracy: 0.7421875\n",
      "Epoch: 152 ..... Loss: 147.05441284179688 ..... Accuracy: 0.7890625\n",
      "Epoch: 153 ..... Loss: 273.2717590332031 ..... Accuracy: 0.7578125\n",
      "Epoch: 154 ..... Loss: 259.99041748046875 ..... Accuracy: 0.7578125\n",
      "Epoch: 155 ..... Loss: 294.56500244140625 ..... Accuracy: 0.7890625\n",
      "Epoch: 156 ..... Loss: 184.90415954589844 ..... Accuracy: 0.828125\n",
      "Epoch: 157 ..... Loss: 248.97927856445312 ..... Accuracy: 0.8203125\n",
      "Epoch: 158 ..... Loss: 207.3223419189453 ..... Accuracy: 0.8359375\n",
      "Epoch: 159 ..... Loss: 294.8948974609375 ..... Accuracy: 0.796875\n",
      "Epoch: 160 ..... Loss: 368.78387451171875 ..... Accuracy: 0.7578125\n",
      "Epoch: 161 ..... Loss: 286.6206970214844 ..... Accuracy: 0.765625\n",
      "Epoch: 162 ..... Loss: 254.9647674560547 ..... Accuracy: 0.765625\n",
      "Epoch: 163 ..... Loss: 283.8850402832031 ..... Accuracy: 0.75\n",
      "Epoch: 164 ..... Loss: 225.41549682617188 ..... Accuracy: 0.78125\n",
      "Epoch: 165 ..... Loss: 236.44247436523438 ..... Accuracy: 0.796875\n",
      "Epoch: 166 ..... Loss: 221.44363403320312 ..... Accuracy: 0.7890625\n",
      "Epoch: 167 ..... Loss: 134.6877899169922 ..... Accuracy: 0.8203125\n",
      "Epoch: 168 ..... Loss: 229.58615112304688 ..... Accuracy: 0.7421875\n",
      "Epoch: 169 ..... Loss: 158.0171356201172 ..... Accuracy: 0.8125\n",
      "Epoch: 170 ..... Loss: 222.5885467529297 ..... Accuracy: 0.78125\n",
      "Epoch: 171 ..... Loss: 379.2242736816406 ..... Accuracy: 0.75\n",
      "Epoch: 172 ..... Loss: 224.01025390625 ..... Accuracy: 0.8125\n",
      "Epoch: 173 ..... Loss: 143.02154541015625 ..... Accuracy: 0.7734375\n",
      "Epoch: 174 ..... Loss: 191.91744995117188 ..... Accuracy: 0.8125\n",
      "Epoch: 175 ..... Loss: 234.063720703125 ..... Accuracy: 0.7890625\n",
      "Epoch: 176 ..... Loss: 88.13233184814453 ..... Accuracy: 0.8359375\n",
      "Epoch: 177 ..... Loss: 248.177734375 ..... Accuracy: 0.78125\n",
      "Epoch: 178 ..... Loss: 199.17025756835938 ..... Accuracy: 0.8515625\n",
      "Epoch: 179 ..... Loss: 286.317626953125 ..... Accuracy: 0.7265625\n",
      "Epoch: 180 ..... Loss: 174.96083068847656 ..... Accuracy: 0.84375\n",
      "Epoch: 181 ..... Loss: 190.77194213867188 ..... Accuracy: 0.8359375\n",
      "Epoch: 182 ..... Loss: 228.78643798828125 ..... Accuracy: 0.78125\n",
      "Epoch: 183 ..... Loss: 216.3891143798828 ..... Accuracy: 0.796875\n",
      "Epoch: 184 ..... Loss: 152.0670928955078 ..... Accuracy: 0.8203125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 185 ..... Loss: 163.3603515625 ..... Accuracy: 0.828125\n",
      "Epoch: 186 ..... Loss: 193.85064697265625 ..... Accuracy: 0.78125\n",
      "Epoch: 187 ..... Loss: 136.1670379638672 ..... Accuracy: 0.84375\n",
      "Epoch: 188 ..... Loss: 189.3018035888672 ..... Accuracy: 0.8125\n",
      "Epoch: 189 ..... Loss: 232.81689453125 ..... Accuracy: 0.7890625\n",
      "Epoch: 190 ..... Loss: 250.13449096679688 ..... Accuracy: 0.8125\n",
      "Epoch: 191 ..... Loss: 163.80303955078125 ..... Accuracy: 0.78125\n",
      "Epoch: 192 ..... Loss: 215.83544921875 ..... Accuracy: 0.7890625\n",
      "Epoch: 193 ..... Loss: 221.798583984375 ..... Accuracy: 0.7578125\n",
      "Epoch: 194 ..... Loss: 156.61004638671875 ..... Accuracy: 0.8515625\n",
      "Epoch: 195 ..... Loss: 172.63729858398438 ..... Accuracy: 0.828125\n",
      "Epoch: 196 ..... Loss: 187.78131103515625 ..... Accuracy: 0.765625\n",
      "Epoch: 197 ..... Loss: 185.86888122558594 ..... Accuracy: 0.828125\n",
      "Epoch: 198 ..... Loss: 194.81039428710938 ..... Accuracy: 0.78125\n",
      "Epoch: 199 ..... Loss: 208.35943603515625 ..... Accuracy: 0.8359375\n",
      "Epoch: 200 ..... Loss: 183.10911560058594 ..... Accuracy: 0.8046875\n",
      "Epoch: 201 ..... Loss: 162.90377807617188 ..... Accuracy: 0.828125\n",
      "Epoch: 202 ..... Loss: 252.80963134765625 ..... Accuracy: 0.8359375\n",
      "Epoch: 203 ..... Loss: 266.0865173339844 ..... Accuracy: 0.765625\n",
      "Epoch: 204 ..... Loss: 201.60687255859375 ..... Accuracy: 0.7734375\n",
      "Epoch: 205 ..... Loss: 165.00927734375 ..... Accuracy: 0.8515625\n",
      "Epoch: 206 ..... Loss: 157.52786254882812 ..... Accuracy: 0.7890625\n",
      "Epoch: 207 ..... Loss: 256.90216064453125 ..... Accuracy: 0.7578125\n",
      "Epoch: 208 ..... Loss: 313.454345703125 ..... Accuracy: 0.7734375\n",
      "Epoch: 209 ..... Loss: 268.2381896972656 ..... Accuracy: 0.8203125\n",
      "Epoch: 210 ..... Loss: 246.1446075439453 ..... Accuracy: 0.78125\n",
      "Epoch: 211 ..... Loss: 214.83990478515625 ..... Accuracy: 0.78125\n",
      "Epoch: 212 ..... Loss: 303.5480041503906 ..... Accuracy: 0.7578125\n",
      "Epoch: 213 ..... Loss: 212.9523468017578 ..... Accuracy: 0.8203125\n",
      "Epoch: 214 ..... Loss: 200.06283569335938 ..... Accuracy: 0.8203125\n",
      "Epoch: 215 ..... Loss: 135.72503662109375 ..... Accuracy: 0.84375\n",
      "Epoch: 216 ..... Loss: 161.57473754882812 ..... Accuracy: 0.8203125\n",
      "Epoch: 217 ..... Loss: 191.75897216796875 ..... Accuracy: 0.8203125\n",
      "Epoch: 218 ..... Loss: 275.6177978515625 ..... Accuracy: 0.75\n",
      "Epoch: 219 ..... Loss: 224.61094665527344 ..... Accuracy: 0.765625\n",
      "Epoch: 220 ..... Loss: 118.70889282226562 ..... Accuracy: 0.8515625\n",
      "Epoch: 221 ..... Loss: 189.91339111328125 ..... Accuracy: 0.78125\n",
      "Epoch: 222 ..... Loss: 202.74603271484375 ..... Accuracy: 0.8046875\n",
      "Epoch: 223 ..... Loss: 147.75588989257812 ..... Accuracy: 0.875\n",
      "Epoch: 224 ..... Loss: 159.982666015625 ..... Accuracy: 0.8046875\n",
      "Epoch: 225 ..... Loss: 207.55758666992188 ..... Accuracy: 0.78125\n",
      "Epoch: 226 ..... Loss: 97.97236633300781 ..... Accuracy: 0.8515625\n",
      "Epoch: 227 ..... Loss: 354.08984375 ..... Accuracy: 0.796875\n",
      "Epoch: 228 ..... Loss: 204.72622680664062 ..... Accuracy: 0.8359375\n",
      "Epoch: 229 ..... Loss: 211.63006591796875 ..... Accuracy: 0.8046875\n",
      "Epoch: 230 ..... Loss: 221.05780029296875 ..... Accuracy: 0.796875\n",
      "Epoch: 231 ..... Loss: 173.46554565429688 ..... Accuracy: 0.84375\n",
      "Epoch: 232 ..... Loss: 82.9050064086914 ..... Accuracy: 0.8828125\n",
      "Epoch: 233 ..... Loss: 248.49293518066406 ..... Accuracy: 0.8125\n",
      "Epoch: 234 ..... Loss: 164.22706604003906 ..... Accuracy: 0.7890625\n",
      "Epoch: 235 ..... Loss: 111.43134307861328 ..... Accuracy: 0.8515625\n",
      "Epoch: 236 ..... Loss: 300.4736328125 ..... Accuracy: 0.765625\n",
      "Epoch: 237 ..... Loss: 168.82208251953125 ..... Accuracy: 0.8515625\n",
      "Epoch: 238 ..... Loss: 219.17208862304688 ..... Accuracy: 0.796875\n",
      "Epoch: 239 ..... Loss: 164.21725463867188 ..... Accuracy: 0.8125\n",
      "Epoch: 240 ..... Loss: 311.65673828125 ..... Accuracy: 0.8203125\n",
      "Epoch: 241 ..... Loss: 241.61773681640625 ..... Accuracy: 0.75\n",
      "Epoch: 242 ..... Loss: 132.83680725097656 ..... Accuracy: 0.7734375\n",
      "Epoch: 243 ..... Loss: 147.8757781982422 ..... Accuracy: 0.875\n",
      "Epoch: 244 ..... Loss: 163.03369140625 ..... Accuracy: 0.8671875\n",
      "Epoch: 245 ..... Loss: 178.0284423828125 ..... Accuracy: 0.8359375\n",
      "Epoch: 246 ..... Loss: 117.29894256591797 ..... Accuracy: 0.875\n",
      "Epoch: 247 ..... Loss: 180.4779815673828 ..... Accuracy: 0.84375\n",
      "Epoch: 248 ..... Loss: 168.07861328125 ..... Accuracy: 0.8046875\n",
      "Epoch: 249 ..... Loss: 232.49290466308594 ..... Accuracy: 0.8125\n",
      "Epoch: 250 ..... Loss: 112.4481430053711 ..... Accuracy: 0.875\n",
      "Epoch: 251 ..... Loss: 233.2974395751953 ..... Accuracy: 0.7890625\n",
      "Epoch: 252 ..... Loss: 71.68209838867188 ..... Accuracy: 0.8828125\n",
      "Epoch: 253 ..... Loss: 65.07093811035156 ..... Accuracy: 0.921875\n",
      "Epoch: 254 ..... Loss: 153.72341918945312 ..... Accuracy: 0.8359375\n",
      "Epoch: 255 ..... Loss: 166.0257568359375 ..... Accuracy: 0.8125\n",
      "Epoch: 256 ..... Loss: 92.9754867553711 ..... Accuracy: 0.8359375\n",
      "Epoch: 257 ..... Loss: 135.07022094726562 ..... Accuracy: 0.859375\n",
      "Epoch: 258 ..... Loss: 124.93060302734375 ..... Accuracy: 0.8203125\n",
      "Epoch: 259 ..... Loss: 167.00619506835938 ..... Accuracy: 0.8515625\n",
      "Epoch: 260 ..... Loss: 131.95767211914062 ..... Accuracy: 0.8671875\n",
      "Epoch: 261 ..... Loss: 216.33436584472656 ..... Accuracy: 0.8046875\n",
      "Epoch: 262 ..... Loss: 207.74183654785156 ..... Accuracy: 0.7890625\n",
      "Epoch: 263 ..... Loss: 143.33148193359375 ..... Accuracy: 0.8359375\n",
      "Epoch: 264 ..... Loss: 192.42684936523438 ..... Accuracy: 0.7890625\n",
      "Epoch: 265 ..... Loss: 276.00146484375 ..... Accuracy: 0.8359375\n",
      "Epoch: 266 ..... Loss: 112.74620056152344 ..... Accuracy: 0.859375\n",
      "Epoch: 267 ..... Loss: 217.523681640625 ..... Accuracy: 0.8203125\n",
      "Epoch: 268 ..... Loss: 213.34051513671875 ..... Accuracy: 0.796875\n",
      "Epoch: 269 ..... Loss: 80.84088897705078 ..... Accuracy: 0.875\n",
      "Epoch: 270 ..... Loss: 172.7161407470703 ..... Accuracy: 0.828125\n",
      "Epoch: 271 ..... Loss: 159.00546264648438 ..... Accuracy: 0.8203125\n",
      "Epoch: 272 ..... Loss: 151.37863159179688 ..... Accuracy: 0.8203125\n",
      "Epoch: 273 ..... Loss: 111.57957458496094 ..... Accuracy: 0.828125\n",
      "Epoch: 274 ..... Loss: 187.1113739013672 ..... Accuracy: 0.78125\n",
      "Epoch: 275 ..... Loss: 189.27517700195312 ..... Accuracy: 0.84375\n",
      "Epoch: 276 ..... Loss: 165.3443603515625 ..... Accuracy: 0.8125\n",
      "Epoch: 277 ..... Loss: 188.3282012939453 ..... Accuracy: 0.8203125\n",
      "Epoch: 278 ..... Loss: 216.4130096435547 ..... Accuracy: 0.7890625\n",
      "Epoch: 279 ..... Loss: 205.30062866210938 ..... Accuracy: 0.8203125\n",
      "Epoch: 280 ..... Loss: 143.09181213378906 ..... Accuracy: 0.8125\n",
      "Epoch: 281 ..... Loss: 113.2465591430664 ..... Accuracy: 0.8359375\n",
      "Epoch: 282 ..... Loss: 189.35867309570312 ..... Accuracy: 0.8125\n",
      "Epoch: 283 ..... Loss: 175.2017822265625 ..... Accuracy: 0.8359375\n",
      "Epoch: 284 ..... Loss: 181.10568237304688 ..... Accuracy: 0.828125\n",
      "Epoch: 285 ..... Loss: 196.31666564941406 ..... Accuracy: 0.78125\n",
      "Epoch: 286 ..... Loss: 196.25653076171875 ..... Accuracy: 0.828125\n",
      "Epoch: 287 ..... Loss: 172.46694946289062 ..... Accuracy: 0.859375\n",
      "Epoch: 288 ..... Loss: 142.03848266601562 ..... Accuracy: 0.890625\n",
      "Epoch: 289 ..... Loss: 227.21124267578125 ..... Accuracy: 0.828125\n",
      "Epoch: 290 ..... Loss: 176.942626953125 ..... Accuracy: 0.8359375\n",
      "Epoch: 291 ..... Loss: 189.19081115722656 ..... Accuracy: 0.8203125\n",
      "Epoch: 292 ..... Loss: 143.40090942382812 ..... Accuracy: 0.8203125\n",
      "Epoch: 293 ..... Loss: 155.5909881591797 ..... Accuracy: 0.828125\n",
      "Epoch: 294 ..... Loss: 116.81216430664062 ..... Accuracy: 0.8671875\n",
      "Epoch: 295 ..... Loss: 125.17756652832031 ..... Accuracy: 0.875\n",
      "Epoch: 296 ..... Loss: 108.03237915039062 ..... Accuracy: 0.875\n",
      "Epoch: 297 ..... Loss: 183.5252685546875 ..... Accuracy: 0.8046875\n",
      "Epoch: 298 ..... Loss: 251.47222900390625 ..... Accuracy: 0.8203125\n",
      "Epoch: 299 ..... Loss: 130.6332550048828 ..... Accuracy: 0.859375\n",
      "Epoch: 300 ..... Loss: 242.07388305664062 ..... Accuracy: 0.8359375\n",
      "Epoch: 301 ..... Loss: 135.71458435058594 ..... Accuracy: 0.859375\n",
      "Epoch: 302 ..... Loss: 208.73756408691406 ..... Accuracy: 0.78125\n",
      "Epoch: 303 ..... Loss: 110.216064453125 ..... Accuracy: 0.8515625\n",
      "Epoch: 304 ..... Loss: 128.71377563476562 ..... Accuracy: 0.8203125\n",
      "Epoch: 305 ..... Loss: 117.6864013671875 ..... Accuracy: 0.8515625\n",
      "Epoch: 306 ..... Loss: 205.19876098632812 ..... Accuracy: 0.828125\n",
      "Epoch: 307 ..... Loss: 107.49118041992188 ..... Accuracy: 0.875\n",
      "Epoch: 308 ..... Loss: 143.724853515625 ..... Accuracy: 0.8125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 309 ..... Loss: 150.68348693847656 ..... Accuracy: 0.7890625\n",
      "Epoch: 310 ..... Loss: 130.3842010498047 ..... Accuracy: 0.828125\n",
      "Epoch: 311 ..... Loss: 112.02979278564453 ..... Accuracy: 0.8125\n",
      "Epoch: 312 ..... Loss: 211.26486206054688 ..... Accuracy: 0.8125\n",
      "Epoch: 313 ..... Loss: 175.78085327148438 ..... Accuracy: 0.8203125\n",
      "Epoch: 314 ..... Loss: 185.52853393554688 ..... Accuracy: 0.78125\n",
      "Epoch: 315 ..... Loss: 214.74008178710938 ..... Accuracy: 0.828125\n",
      "Epoch: 316 ..... Loss: 130.60311889648438 ..... Accuracy: 0.8203125\n",
      "Epoch: 317 ..... Loss: 136.44171142578125 ..... Accuracy: 0.84375\n",
      "Epoch: 318 ..... Loss: 98.7204818725586 ..... Accuracy: 0.84375\n",
      "Epoch: 319 ..... Loss: 160.7425079345703 ..... Accuracy: 0.8046875\n",
      "Epoch: 320 ..... Loss: 81.76654052734375 ..... Accuracy: 0.9140625\n",
      "Epoch: 321 ..... Loss: 165.97799682617188 ..... Accuracy: 0.8359375\n",
      "Epoch: 322 ..... Loss: 155.30709838867188 ..... Accuracy: 0.84375\n",
      "Epoch: 323 ..... Loss: 134.761962890625 ..... Accuracy: 0.875\n",
      "Epoch: 324 ..... Loss: 97.81360626220703 ..... Accuracy: 0.8671875\n",
      "Epoch: 325 ..... Loss: 208.15435791015625 ..... Accuracy: 0.78125\n",
      "Epoch: 326 ..... Loss: 251.6352081298828 ..... Accuracy: 0.7578125\n",
      "Epoch: 327 ..... Loss: 205.953369140625 ..... Accuracy: 0.8203125\n",
      "Epoch: 328 ..... Loss: 121.17902374267578 ..... Accuracy: 0.7890625\n",
      "Epoch: 329 ..... Loss: 203.57608032226562 ..... Accuracy: 0.8125\n",
      "Epoch: 330 ..... Loss: 53.76259231567383 ..... Accuracy: 0.890625\n",
      "Epoch: 331 ..... Loss: 159.3815155029297 ..... Accuracy: 0.828125\n",
      "Epoch: 332 ..... Loss: 279.2462158203125 ..... Accuracy: 0.78125\n",
      "Epoch: 333 ..... Loss: 163.61874389648438 ..... Accuracy: 0.8125\n",
      "Epoch: 334 ..... Loss: 185.32008361816406 ..... Accuracy: 0.8203125\n",
      "Epoch: 335 ..... Loss: 174.63973999023438 ..... Accuracy: 0.828125\n",
      "Epoch: 336 ..... Loss: 207.84768676757812 ..... Accuracy: 0.8515625\n",
      "Epoch: 337 ..... Loss: 104.92799377441406 ..... Accuracy: 0.859375\n",
      "Epoch: 338 ..... Loss: 129.7143096923828 ..... Accuracy: 0.84375\n",
      "Epoch: 339 ..... Loss: 153.31300354003906 ..... Accuracy: 0.8359375\n",
      "Epoch: 340 ..... Loss: 170.91241455078125 ..... Accuracy: 0.7578125\n",
      "Epoch: 341 ..... Loss: 136.6990509033203 ..... Accuracy: 0.828125\n",
      "Epoch: 342 ..... Loss: 107.94371795654297 ..... Accuracy: 0.875\n",
      "Epoch: 343 ..... Loss: 134.53195190429688 ..... Accuracy: 0.8515625\n",
      "Epoch: 344 ..... Loss: 107.70661163330078 ..... Accuracy: 0.890625\n",
      "Epoch: 345 ..... Loss: 118.4599838256836 ..... Accuracy: 0.8515625\n",
      "Epoch: 346 ..... Loss: 128.93746948242188 ..... Accuracy: 0.8671875\n",
      "Epoch: 347 ..... Loss: 101.73396301269531 ..... Accuracy: 0.84375\n",
      "Epoch: 348 ..... Loss: 144.15106201171875 ..... Accuracy: 0.8203125\n",
      "Epoch: 349 ..... Loss: 146.07858276367188 ..... Accuracy: 0.8515625\n",
      "Epoch: 350 ..... Loss: 146.97079467773438 ..... Accuracy: 0.8203125\n",
      "Epoch: 351 ..... Loss: 170.5554656982422 ..... Accuracy: 0.8359375\n",
      "Epoch: 352 ..... Loss: 113.76339721679688 ..... Accuracy: 0.859375\n",
      "Epoch: 353 ..... Loss: 226.57626342773438 ..... Accuracy: 0.8046875\n",
      "Epoch: 354 ..... Loss: 88.90658569335938 ..... Accuracy: 0.8515625\n",
      "Epoch: 355 ..... Loss: 125.3075942993164 ..... Accuracy: 0.859375\n",
      "Epoch: 356 ..... Loss: 115.18614196777344 ..... Accuracy: 0.84375\n",
      "Epoch: 357 ..... Loss: 184.84335327148438 ..... Accuracy: 0.8203125\n",
      "Epoch: 358 ..... Loss: 137.2965545654297 ..... Accuracy: 0.84375\n",
      "Epoch: 359 ..... Loss: 119.08323669433594 ..... Accuracy: 0.8515625\n",
      "Epoch: 360 ..... Loss: 128.9375 ..... Accuracy: 0.828125\n",
      "Epoch: 361 ..... Loss: 199.51080322265625 ..... Accuracy: 0.8359375\n",
      "Epoch: 362 ..... Loss: 148.78175354003906 ..... Accuracy: 0.84375\n",
      "Epoch: 363 ..... Loss: 97.05587005615234 ..... Accuracy: 0.875\n",
      "Epoch: 364 ..... Loss: 227.991943359375 ..... Accuracy: 0.8515625\n",
      "Epoch: 365 ..... Loss: 172.09463500976562 ..... Accuracy: 0.8125\n",
      "Epoch: 366 ..... Loss: 216.7346954345703 ..... Accuracy: 0.8203125\n",
      "Epoch: 367 ..... Loss: 151.7637939453125 ..... Accuracy: 0.8359375\n",
      "Epoch: 368 ..... Loss: 101.13833618164062 ..... Accuracy: 0.8125\n",
      "Epoch: 369 ..... Loss: 164.66754150390625 ..... Accuracy: 0.7734375\n",
      "Epoch: 370 ..... Loss: 143.7438201904297 ..... Accuracy: 0.8359375\n",
      "Epoch: 371 ..... Loss: 168.544921875 ..... Accuracy: 0.8515625\n",
      "Epoch: 372 ..... Loss: 218.5152130126953 ..... Accuracy: 0.8046875\n",
      "Epoch: 373 ..... Loss: 118.53109741210938 ..... Accuracy: 0.875\n",
      "Epoch: 374 ..... Loss: 106.13631439208984 ..... Accuracy: 0.90625\n",
      "Epoch: 375 ..... Loss: 85.21107482910156 ..... Accuracy: 0.90625\n",
      "Epoch: 376 ..... Loss: 119.16847229003906 ..... Accuracy: 0.8671875\n",
      "Epoch: 377 ..... Loss: 178.9036865234375 ..... Accuracy: 0.8046875\n",
      "Epoch: 378 ..... Loss: 175.1772003173828 ..... Accuracy: 0.8125\n",
      "Epoch: 379 ..... Loss: 116.96891784667969 ..... Accuracy: 0.84375\n",
      "Epoch: 380 ..... Loss: 115.34783172607422 ..... Accuracy: 0.84375\n",
      "Epoch: 381 ..... Loss: 136.91827392578125 ..... Accuracy: 0.8125\n",
      "Epoch: 382 ..... Loss: 146.90896606445312 ..... Accuracy: 0.8359375\n",
      "Epoch: 383 ..... Loss: 133.37420654296875 ..... Accuracy: 0.859375\n",
      "Epoch: 384 ..... Loss: 170.6990966796875 ..... Accuracy: 0.8125\n",
      "Epoch: 385 ..... Loss: 197.82080078125 ..... Accuracy: 0.8046875\n",
      "Epoch: 386 ..... Loss: 104.19483184814453 ..... Accuracy: 0.84375\n",
      "Epoch: 387 ..... Loss: 173.62509155273438 ..... Accuracy: 0.8046875\n",
      "Epoch: 388 ..... Loss: 158.01528930664062 ..... Accuracy: 0.875\n",
      "Epoch: 389 ..... Loss: 143.29400634765625 ..... Accuracy: 0.8203125\n",
      "Epoch: 390 ..... Loss: 127.14369201660156 ..... Accuracy: 0.8671875\n",
      "Epoch: 391 ..... Loss: 126.03142547607422 ..... Accuracy: 0.8671875\n",
      "Epoch: 392 ..... Loss: 145.32052612304688 ..... Accuracy: 0.8203125\n",
      "Epoch: 393 ..... Loss: 178.28900146484375 ..... Accuracy: 0.796875\n",
      "Epoch: 394 ..... Loss: 133.46693420410156 ..... Accuracy: 0.828125\n",
      "Epoch: 395 ..... Loss: 133.4077606201172 ..... Accuracy: 0.890625\n",
      "Epoch: 396 ..... Loss: 95.71572875976562 ..... Accuracy: 0.90625\n",
      "Epoch: 397 ..... Loss: 134.48251342773438 ..... Accuracy: 0.875\n",
      "Epoch: 398 ..... Loss: 117.56627655029297 ..... Accuracy: 0.8359375\n",
      "Epoch: 399 ..... Loss: 183.0050048828125 ..... Accuracy: 0.8359375\n",
      "Epoch: 400 ..... Loss: 101.0414047241211 ..... Accuracy: 0.9140625\n",
      "Epoch: 401 ..... Loss: 107.56044006347656 ..... Accuracy: 0.890625\n",
      "Epoch: 402 ..... Loss: 117.88296508789062 ..... Accuracy: 0.859375\n",
      "Epoch: 403 ..... Loss: 116.6999740600586 ..... Accuracy: 0.859375\n",
      "Epoch: 404 ..... Loss: 241.65811157226562 ..... Accuracy: 0.7734375\n",
      "Epoch: 405 ..... Loss: 90.68492126464844 ..... Accuracy: 0.8515625\n",
      "Epoch: 406 ..... Loss: 144.85641479492188 ..... Accuracy: 0.8359375\n",
      "Epoch: 407 ..... Loss: 160.85723876953125 ..... Accuracy: 0.8359375\n",
      "Epoch: 408 ..... Loss: 128.32534790039062 ..... Accuracy: 0.875\n",
      "Epoch: 409 ..... Loss: 128.71630859375 ..... Accuracy: 0.84375\n",
      "Epoch: 410 ..... Loss: 145.8215789794922 ..... Accuracy: 0.84375\n",
      "Epoch: 411 ..... Loss: 113.14276885986328 ..... Accuracy: 0.875\n",
      "Epoch: 412 ..... Loss: 86.04957580566406 ..... Accuracy: 0.859375\n",
      "Epoch: 413 ..... Loss: 153.9613037109375 ..... Accuracy: 0.8671875\n",
      "Epoch: 414 ..... Loss: 76.34349060058594 ..... Accuracy: 0.890625\n",
      "Epoch: 415 ..... Loss: 150.15615844726562 ..... Accuracy: 0.8515625\n",
      "Epoch: 416 ..... Loss: 155.317138671875 ..... Accuracy: 0.8671875\n",
      "Epoch: 417 ..... Loss: 211.3422088623047 ..... Accuracy: 0.7890625\n",
      "Epoch: 418 ..... Loss: 254.87734985351562 ..... Accuracy: 0.8203125\n",
      "Epoch: 419 ..... Loss: 46.37565994262695 ..... Accuracy: 0.9140625\n",
      "Epoch: 420 ..... Loss: 84.57502746582031 ..... Accuracy: 0.859375\n",
      "Epoch: 421 ..... Loss: 114.2285385131836 ..... Accuracy: 0.859375\n",
      "Epoch: 422 ..... Loss: 145.0746307373047 ..... Accuracy: 0.8359375\n",
      "Epoch: 423 ..... Loss: 92.47593688964844 ..... Accuracy: 0.8515625\n",
      "Epoch: 424 ..... Loss: 116.1236572265625 ..... Accuracy: 0.8359375\n",
      "Epoch: 425 ..... Loss: 162.57351684570312 ..... Accuracy: 0.875\n",
      "Epoch: 426 ..... Loss: 68.46678161621094 ..... Accuracy: 0.8828125\n",
      "Epoch: 427 ..... Loss: 119.198486328125 ..... Accuracy: 0.8359375\n",
      "Epoch: 428 ..... Loss: 160.98907470703125 ..... Accuracy: 0.8046875\n",
      "Epoch: 429 ..... Loss: 139.1280517578125 ..... Accuracy: 0.8359375\n",
      "Epoch: 430 ..... Loss: 74.28401947021484 ..... Accuracy: 0.9296875\n",
      "Epoch: 431 ..... Loss: 146.61727905273438 ..... Accuracy: 0.8125\n",
      "Epoch: 432 ..... Loss: 127.49141693115234 ..... Accuracy: 0.8046875\n",
      "Epoch: 433 ..... Loss: 125.51797485351562 ..... Accuracy: 0.8359375\n",
      "Epoch: 434 ..... Loss: 127.35963439941406 ..... Accuracy: 0.8671875\n",
      "Epoch: 435 ..... Loss: 96.66864013671875 ..... Accuracy: 0.8515625\n",
      "Epoch: 436 ..... Loss: 94.46218872070312 ..... Accuracy: 0.8828125\n",
      "Epoch: 437 ..... Loss: 225.85342407226562 ..... Accuracy: 0.7890625\n",
      "Epoch: 438 ..... Loss: 50.338958740234375 ..... Accuracy: 0.8984375\n",
      "Epoch: 439 ..... Loss: 161.8546142578125 ..... Accuracy: 0.859375\n",
      "Epoch: 440 ..... Loss: 138.62510681152344 ..... Accuracy: 0.8046875\n",
      "Epoch: 441 ..... Loss: 131.95973205566406 ..... Accuracy: 0.84375\n",
      "Epoch: 442 ..... Loss: 79.11004638671875 ..... Accuracy: 0.875\n",
      "Epoch: 443 ..... Loss: 156.4786834716797 ..... Accuracy: 0.8515625\n",
      "Epoch: 444 ..... Loss: 161.518310546875 ..... Accuracy: 0.8203125\n",
      "Epoch: 445 ..... Loss: 123.99459838867188 ..... Accuracy: 0.8671875\n",
      "Epoch: 446 ..... Loss: 174.2186279296875 ..... Accuracy: 0.78125\n",
      "Epoch: 447 ..... Loss: 136.0380096435547 ..... Accuracy: 0.8515625\n",
      "Epoch: 448 ..... Loss: 154.5707244873047 ..... Accuracy: 0.84375\n",
      "Epoch: 449 ..... Loss: 79.1328125 ..... Accuracy: 0.859375\n",
      "Epoch: 450 ..... Loss: 112.62007141113281 ..... Accuracy: 0.8671875\n",
      "Epoch: 451 ..... Loss: 164.49395751953125 ..... Accuracy: 0.8828125\n",
      "Epoch: 452 ..... Loss: 137.51055908203125 ..... Accuracy: 0.84375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 453 ..... Loss: 127.88916778564453 ..... Accuracy: 0.8515625\n",
      "Epoch: 454 ..... Loss: 74.10942840576172 ..... Accuracy: 0.875\n",
      "Epoch: 455 ..... Loss: 134.20086669921875 ..... Accuracy: 0.8125\n",
      "Epoch: 456 ..... Loss: 71.04161834716797 ..... Accuracy: 0.8671875\n",
      "Epoch: 457 ..... Loss: 142.5675811767578 ..... Accuracy: 0.8515625\n",
      "Epoch: 458 ..... Loss: 116.24615478515625 ..... Accuracy: 0.8828125\n",
      "Epoch: 459 ..... Loss: 145.6371612548828 ..... Accuracy: 0.859375\n",
      "Epoch: 460 ..... Loss: 112.15884399414062 ..... Accuracy: 0.8359375\n",
      "Epoch: 461 ..... Loss: 95.66786193847656 ..... Accuracy: 0.828125\n",
      "Epoch: 462 ..... Loss: 119.57940673828125 ..... Accuracy: 0.8515625\n",
      "Epoch: 463 ..... Loss: 123.815673828125 ..... Accuracy: 0.8671875\n",
      "Epoch: 464 ..... Loss: 180.89840698242188 ..... Accuracy: 0.84375\n",
      "Epoch: 465 ..... Loss: 52.463890075683594 ..... Accuracy: 0.8984375\n",
      "Epoch: 466 ..... Loss: 74.83309936523438 ..... Accuracy: 0.8203125\n",
      "Epoch: 467 ..... Loss: 60.42491149902344 ..... Accuracy: 0.8828125\n",
      "Epoch: 468 ..... Loss: 93.38507080078125 ..... Accuracy: 0.875\n",
      "Epoch: 469 ..... Loss: 177.171875 ..... Accuracy: 0.8515625\n",
      "Epoch: 470 ..... Loss: 115.52589416503906 ..... Accuracy: 0.8515625\n",
      "Epoch: 471 ..... Loss: 125.41056823730469 ..... Accuracy: 0.828125\n",
      "Epoch: 472 ..... Loss: 87.9443130493164 ..... Accuracy: 0.875\n",
      "Epoch: 473 ..... Loss: 165.6900634765625 ..... Accuracy: 0.828125\n",
      "Epoch: 474 ..... Loss: 131.62225341796875 ..... Accuracy: 0.875\n",
      "Epoch: 475 ..... Loss: 56.686546325683594 ..... Accuracy: 0.8984375\n",
      "Epoch: 476 ..... Loss: 99.26174926757812 ..... Accuracy: 0.890625\n",
      "Epoch: 477 ..... Loss: 125.23750305175781 ..... Accuracy: 0.8828125\n",
      "Epoch: 478 ..... Loss: 71.8900146484375 ..... Accuracy: 0.890625\n",
      "Epoch: 479 ..... Loss: 84.98297882080078 ..... Accuracy: 0.890625\n",
      "Epoch: 480 ..... Loss: 81.39601135253906 ..... Accuracy: 0.890625\n",
      "Epoch: 481 ..... Loss: 74.25755310058594 ..... Accuracy: 0.8828125\n",
      "Epoch: 482 ..... Loss: 149.36685180664062 ..... Accuracy: 0.84375\n",
      "Epoch: 483 ..... Loss: 144.0203857421875 ..... Accuracy: 0.8125\n",
      "Epoch: 484 ..... Loss: 88.82240295410156 ..... Accuracy: 0.859375\n",
      "Epoch: 485 ..... Loss: 158.29208374023438 ..... Accuracy: 0.875\n",
      "Epoch: 486 ..... Loss: 119.71376037597656 ..... Accuracy: 0.8671875\n",
      "Epoch: 487 ..... Loss: 163.3245849609375 ..... Accuracy: 0.7890625\n",
      "Epoch: 488 ..... Loss: 72.86971282958984 ..... Accuracy: 0.859375\n",
      "Epoch: 489 ..... Loss: 111.33595275878906 ..... Accuracy: 0.8515625\n",
      "Epoch: 490 ..... Loss: 107.99790954589844 ..... Accuracy: 0.875\n",
      "Epoch: 491 ..... Loss: 170.13858032226562 ..... Accuracy: 0.890625\n",
      "Epoch: 492 ..... Loss: 74.25775146484375 ..... Accuracy: 0.8828125\n",
      "Epoch: 493 ..... Loss: 102.28436279296875 ..... Accuracy: 0.90625\n",
      "Epoch: 494 ..... Loss: 116.63311767578125 ..... Accuracy: 0.8359375\n",
      "Epoch: 495 ..... Loss: 140.2261962890625 ..... Accuracy: 0.828125\n",
      "Epoch: 496 ..... Loss: 105.6103515625 ..... Accuracy: 0.8828125\n",
      "Epoch: 497 ..... Loss: 97.10233306884766 ..... Accuracy: 0.84375\n",
      "Epoch: 498 ..... Loss: 141.72865295410156 ..... Accuracy: 0.8515625\n",
      "Epoch: 499 ..... Loss: 88.4805908203125 ..... Accuracy: 0.84375\n",
      "Epoch: 500 ..... Loss: 118.91038513183594 ..... Accuracy: 0.8671875\n",
      "Overall Accuracy: 0.8529999852180481\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        sess.run(optimizer, feed_dict = {X: batch_xs, Y:batch_ys})\n",
    "        loss_,acc = sess.run([loss, accuracy], feed_dict={X: batch_xs, Y:batch_ys})\n",
    "        print(\"Epoch: {} ..... Loss: {} ..... Accuracy: {}\".format(epoch+1, loss_, acc))\n",
    "        \n",
    "    print(\"Overall Accuracy: {}\".format(sess.run(accuracy, feed_dict={X:mnist.test.images, Y:mnist.test.labels})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
